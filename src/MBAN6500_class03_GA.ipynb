{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "MBAN6500_class03_GA.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYBaM2duNz06"
   },
   "source": [
    "# Genetic algorithms tested on Frozen Lake \n",
    "\n",
    "This tutorial will use the [OpenAI gym](https://gym.openai.com/) to test an evolutionary computation algorithm. Gym is a toolkit for developing and comparing reinforcement learning and genetic algorithms. It makes no assumptions about the structure of your agent, and is compatible with, for example, TensorFlow. We will later use it to study reinforcement learning. \n",
    "\n",
    "The gym is a collection of test problems -- environments -- that you can use to work out your algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "\n",
    "## Installation\n",
    "If you run this on your local machine then you will probably need to install ```gym```, however on ```gym``` seems to be pre-installed on Colab.\n",
    "```\n",
    "pip install gym\n",
    "```\n",
    "\n",
    "## The Frozen Lake problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "    SFFF       (S: starting point, safe)\n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole, fall to your doom)\n",
    "    HFFG       (G: goal, where the frisbee is located)\n",
    "    \n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Both the set of actions and states are discrete sets. The set of states ```S``` includes the possible locations in the grid (1 to 16). The set of actions ```A``` includes possibles moves: \n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "\n",
    "\n",
    "See the Frozen Lake problem at [the OpenAI gym](https://gym.openai.com/envs/FrozenLake-v0/).\n",
    "\n",
    "### Question 1. \n",
    "In every position/state you have 4 possible actions. Also, assume that you pass through every state on your way to the goal. That is, from start (```S```) to goal (```G```) is a sequence of 16 states (including the end points).\n",
    "\n",
    "A set of actions given the states is called a **policy**.\n",
    "\n",
    "* How many possible policies are there in Frozen Lake?\n",
    "\n",
    "Assume that it takes approximately 1 ms to evaluate each sequence.\n",
    "\n",
    "* How many hours would it take to find the best policy via a brute force search?\n",
    "* How is the Frozen Lake problem related to planning?\n",
    "\n",
    "## Take a look at the gym environment\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5gjgWBLjNz08"
   },
   "source": [
    "import gym\n",
    "# get an instance of the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "# reset to start state\n",
    "env.reset()\n",
    "# render the env for inspection\n",
    "env.render()\n",
    "# the square with red? background color is where you're currently located."
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZxztZ3lGNz1N"
   },
   "source": [
    "env.reset()\n",
    "env.step(3)\n",
    "env.render()"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqSzV_uENz1W"
   },
   "source": [
    "* Take an action ```env.step(action)```, render again and observe the behavior.\n",
    "\n",
    "### Question 2. \n",
    "* ```env.step()``` returns some values. What are those?\n",
    "\n",
    "### Question 3. \n",
    "* Take and render multiple steps. Is the environment deterministic or stochastic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqO2NlOBNz1X"
   },
   "source": [
    "## Random search\n",
    "\n",
    "The cell below generates a set of 2000 random solutions and evaluates them. Depending on your answer above, you might want to do a brute force search; just change the variable ```n_policies``` below.\n",
    "\n",
    "### Question 4. \n",
    "Run the cell 10 times and record the best score each time.\n",
    "What is the best score you can got?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yl_wdTpZNz1Z"
   },
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "\n",
    "def run_episode(env, policy, episode_len=100):\n",
    "    \"\"\"\n",
    "    env         : an instance of the gym environment \"FrozenLake-v0\"\n",
    "    policy      : a set of actions to perform in a given state/position\n",
    "    episode_len : upper limit on how many steps/actions can be taken\n",
    "    \"\"\"\n",
    "    total_reward = 0.\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Runs n_episodes of a policy and returns the average reward.\n",
    "    \n",
    "    env         : an instance of the gym environment \"FrozenLake-v0\"\n",
    "    policy      : a set of actions to perform in a given state/position\n",
    "    n_episodes  : the number of episodes to run the policy\n",
    "    \"\"\"\n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    \"\"\"\n",
    "    Generates a random policy of 4 actions in 16 states/position\n",
    "    \"\"\"\n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "## Policy search\n",
    "n_policies = 2000\n",
    "start = time.time()\n",
    "policy_set = []\n",
    "for _ in range(n_policies):\n",
    "    policy_set.append(gen_random_policy())\n",
    "policy_scores = [evaluate_policy(env, p) for p in policy_set]\n",
    "end = time.time()\n",
    "\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" % (np.max(policy_scores) , end - start))"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.29. Time taken = 48.0800 seconds\n",
      "Best score = 0.39. Time taken = 40.4485 seconds\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9cCWTtzNz1l"
   },
   "source": [
    "## Genetic Algorithm -- ONLY Crossover\n",
    "\n",
    "Improve your random search with crossover.\n",
    "\n",
    "### Question 5.\n",
    "Implement the function ```crossover()``` in the cell below. Tip: try to adapt code from the lecture notes or slides.\n",
    "\n",
    "* What score do you get now?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IzcfA07GNz1s"
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "def run_episode(env, policy, episode_len=100):\n",
    "    \"\"\"\n",
    "    Same as above\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Same as above\n",
    "    \"\"\"    \n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    \"\"\"\n",
    "    Same as above\n",
    "    \"\"\"    \n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "def crossover(policy1, policy2):\n",
    "    \"\"\"\n",
    "    Single Point Random Crossover\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    policy1: parent 1\n",
    "    policy2: parent 2\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    child_policy: offspring\n",
    "    \"\"\"\n",
    "    child_policy = policy1.copy()\n",
    "    j = np.random.randint(1, len(policy1) -2)\n",
    "    child_policy[j:] = policy2[j:]\n",
    "    \n",
    "    # IMPLEMENT!\n",
    "    # generate a child policy from cross-over of the parents\n",
    "\n",
    "    return child_policy\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.seed(0)\n",
    "\n",
    "## Policy search\n",
    "n_policy = 100\n",
    "n_gens = 20 # number of generations\n",
    "start = time.time()\n",
    "\n",
    "# initialize the population with random policies, i.e. a policy population\n",
    "policy_pop = []\n",
    "for _ in range(n_policy):\n",
    "    policy_pop.append(gen_random_policy())\n",
    "\n",
    "for gen in range(1, n_gens+1):\n",
    "    \n",
    "    # evaluate fitness of the policies\n",
    "    policy_scores = []\n",
    "    for policy in policy_pop:\n",
    "        policy_scores.append(evaluate_policy(env, policy))    \n",
    "\n",
    "    # Print the best score of each generation to see improvement\n",
    "    print('Generation %d: max score = %0.2f' %(gen, max(policy_scores)))\n",
    "    \n",
    "    # rank the policies based on fitness\n",
    "    policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "    # select the top 5 policies: no cross-over\n",
    "    elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "    # get selection probabilities based on the policy scores\n",
    "    select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
    "    \n",
    "    child_set = []\n",
    "    for i in range(n_policy - 5):\n",
    "        # randomly select parents weighted towards high select_probs\n",
    "        parent1 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        parent2 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        # generate a child from cross-over btw parents\n",
    "        child = crossover(parent1, parent2)\n",
    "        child_set.append(child)  \n",
    "\n",
    "    # add the elite set (no cross-over) to the new policy population\n",
    "    policy_pop = elite_set\n",
    "    # add the child set\n",
    "    policy_pop += child_set\n",
    "\n",
    "# Get the final policy scores after n_gens of selection and cross-over\n",
    "policy_scores = []\n",
    "for policy in policy_pop:\n",
    "    policy_scores.append(evaluate_policy(env, policy))\n",
    "    \n",
    "# Select the best policy\n",
    "best_policy = policy_pop[np.argmax(policy_scores)]\n",
    "\n",
    "end = time.time()\n",
    "print('Best policy score = %0.2f. Time taken = %4.4f' %(np.max(policy_scores), (end-start)))    \n",
    "\n",
    "## Evaluation\n",
    "env = wrappers.Monitor(env, '/tmp/frozenlake1', force=True)\n",
    "for _ in range(200):\n",
    "    run_episode(env, best_policy)\n",
    "env.close()"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: max score = 0.26\n",
      "Generation 2: max score = 0.33\n",
      "Generation 3: max score = 0.41\n",
      "Generation 4: max score = 0.47\n",
      "Generation 5: max score = 0.52\n",
      "Generation 6: max score = 0.55\n",
      "Generation 7: max score = 0.66\n",
      "Generation 8: max score = 0.71\n",
      "Generation 9: max score = 0.66\n",
      "Generation 10: max score = 0.66\n",
      "Generation 11: max score = 0.72\n",
      "Generation 12: max score = 0.67\n",
      "Generation 13: max score = 0.73\n",
      "Generation 14: max score = 0.71\n",
      "Generation 15: max score = 0.71\n",
      "Generation 16: max score = 0.72\n",
      "Generation 17: max score = 0.72\n",
      "Generation 18: max score = 0.71\n",
      "Generation 19: max score = 0.72\n",
      "Generation 20: max score = 0.72\n",
      "Best policy score = 0.72. Time taken = 116.8944\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDc-03SZNz1y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Genetic Algorithm -- Crossover AND Mutation\n",
    "\n",
    "### Question 5.\n",
    "* Implement the function ```mutation()``` in the code below. You can implement either flip, swap, scramble or inversion.\n",
    "\n",
    "* Does the performance increase further?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hv3i6qhcNz1z"
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "def run_episode(env, policy, episode_len=100):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "        \n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "\n",
    "def gen_random_policy():\n",
    "    \n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "\n",
    "def crossover(policy1, policy2):\n",
    "    \"\"\"\n",
    "    single point random crossover\n",
    "    Arguments\n",
    "    ----------\n",
    "    policy1: parent 1\n",
    "    policy2: parent 2\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    child_policy: offspring\n",
    "    \"\"\"\n",
    "    child_policy = policy1.copy()\n",
    "    # IMPLEMENT!\n",
    "    # generate a child policy from cross-over of the parents\n",
    "    j = np.random.randint(1, len(policy1) -2)\n",
    "    child_policy[j:] = policy2[j:]\n",
    "\n",
    "    return child_policy\n",
    "\n",
    "\n",
    "def mutation(policy, symbols=[0, 1, 2, 3], p=0.05):\n",
    "    \"\"\"\n",
    "    swap mutation\n",
    "    Arguments\n",
    "    ---------\n",
    "    policy -- policy to be mutated\n",
    "    p      -- mutation probability\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mutated_policy -- a mutated version of 'policy'\n",
    "    \"\"\"\n",
    "    #mutated_policy = policy.copy()\n",
    "    ids = np.random.randint(1, len(policy) -1, 2)\n",
    "    policy[ids[0]], policy[ids[1]] = policy[ids[1]], policy[ids[0]]\n",
    "    # IMPLEMENT!\n",
    "    # generate a mutated policy.    \n",
    "            \n",
    "    return policy\n",
    "\n",
    "#set gym env\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.seed(0)\n",
    "\n",
    "## Policy search\n",
    "n_policy = 100\n",
    "n_gens = 20 # number of generations\n",
    "start = time.time()\n",
    "\n",
    "# initialize the population with random policies, i.e. a policy population\n",
    "policy_pop = []\n",
    "for _ in range(n_policy):\n",
    "    policy_pop.append(gen_random_policy())\n",
    "\n",
    "for gen in range(1, n_gens+1):\n",
    "    \n",
    "    # evaluate fitness of the policies\n",
    "    policy_scores = []\n",
    "    for policy in policy_pop:\n",
    "        policy_scores.append(evaluate_policy(env, policy))    \n",
    "        \n",
    "    # Print the best score of each generation to see improvement\n",
    "    print('Generation %d: max score = %0.2f' %(gen, max(policy_scores)))\n",
    "    \n",
    "    # rank the policies based on fitness\n",
    "    policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "    # select the top 5 policies: no cross-over\n",
    "    elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "    # get selection probabilities based on the policy scores\n",
    "    select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
    "    \n",
    "    \n",
    "    child_set = []\n",
    "    for i in range(n_policy - 5):\n",
    "        # randomly select parents weighted towards high select_probs\n",
    "        parent1 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        parent2 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        # generate a child from cross-over btw parents\n",
    "        child = crossover(parent1, parent2)\n",
    "        child_set.append(child)  \n",
    "        \n",
    "    # mutate the children\n",
    "    mutated_list = []\n",
    "    for child in child_set:\n",
    "        mutated = mutation(child)\n",
    "        mutated_list.append(mutated)\n",
    "\n",
    "    # add the elite set (no cross-over) to the new policy population\n",
    "    policy_pop = elite_set\n",
    "        # add the mutated child set\n",
    "    policy_pop += mutated_list\n",
    "\n",
    "policy_scores = []\n",
    "for policy in policy_pop:\n",
    "    policy_scores.append(evaluate_policy(env, policy))\n",
    "    \n",
    "best_policy = policy_pop[np.argmax(policy_scores)]\n",
    "\n",
    "end = time.time()\n",
    "print('Best policy score = %0.2f. Time taken = %4.4f'\n",
    "        %(np.max(policy_scores), (end-start)))    \n",
    "\n",
    "## Evaluation\n",
    "env = wrappers.Monitor(env, '/tmp/frozenlake1', force=True)\n",
    "for _ in range(200):\n",
    "    run_episode(env, best_policy)\n",
    "env.close()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: max score = 0.26\n",
      "Generation 2: max score = 0.26\n",
      "Generation 3: max score = 0.48\n",
      "Generation 4: max score = 0.58\n",
      "Generation 5: max score = 0.65\n",
      "Generation 6: max score = 0.62\n",
      "Generation 7: max score = 0.80\n",
      "Generation 8: max score = 0.73\n",
      "Generation 9: max score = 0.78\n",
      "Generation 10: max score = 0.80\n",
      "Generation 11: max score = 0.78\n",
      "Generation 12: max score = 0.79\n",
      "Generation 13: max score = 0.78\n",
      "Generation 14: max score = 0.76\n",
      "Generation 15: max score = 0.77\n",
      "Generation 16: max score = 0.77\n",
      "Generation 17: max score = 0.81\n",
      "Generation 18: max score = 0.80\n",
      "Generation 19: max score = 0.78\n",
      "Generation 20: max score = 0.80\n",
      "Best policy score = 0.80. Time taken = 112.5857\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSbb2FKRNz13"
   },
   "source": [
    "## Genetic Algorithm -- Crossover AND Mutation AND diversity\n",
    "\n",
    "### Question 6 (optional).\n",
    "* Implement the function an alternative to ```evaluate_policy()``` called ```evaluate_policy_diversity_fitness()``` that combines genetic diversity (see lecture slides) with fitness. Keep ```evaluate_policy()``` but rename it to ```evaluate_policy_fitness()```, it'll be useful for the final evaluation.\n",
    "* Does the performance increase further?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "evU7N0rmNz14",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "def run_episode(env, policy, episode_len=100):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def mean_squared_error(policy1, policy2):\n",
    "    \"\"\"\n",
    "    calculate the mean squared error between two policies\n",
    "    :param policy1:\n",
    "    :param policy2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    delta = np.subtract(policy1, policy2)\n",
    "    return np.square(delta).mean()\n",
    "\n",
    "\n",
    "def max_mean_squared_error(prev_policy_pop):\n",
    "    \"\"\"\n",
    "    get the maximum mean squared error for a policy group.\n",
    "    to be used in normalize diversity score\n",
    "    :param prev_policy_pop:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for p in prev_policy_pop:\n",
    "        total += np.square(p).mean()\n",
    "    return total\n",
    "\n",
    "def evaluate_diversity(policy, prev_policy_pop):\n",
    "    \"\"\"\n",
    "    calculate the diversity score of a policy against a policy population using simple mean squared error\n",
    "    :param policy:  targeted policy\n",
    "    :param prev_policy_pop:  policy population\n",
    "    :return: normalized diversity score\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for p in prev_policy_pop:\n",
    "        total += mean_squared_error(policy, p)\n",
    "    return total / max_mean_squared_error(prev_policy_pop)\n",
    "\n",
    "\n",
    "def evaluate_policy_diversity_fitness(env, policy, prev_policy_pop):\n",
    "    \"\"\"\n",
    "    evaluate both the fitness and diversity of a policy with coefficient = 0.5\n",
    "    score  = 0.5 fitness + 0.5 diversity\n",
    "    \"\"\"\n",
    "    fitness = evaluate_policy_fitness(env, policy)\n",
    "    diversity = evaluate_diversity(policy, prev_policy_pop)\n",
    "    return 0.5 * fitness + 0.5 * diversity\n",
    "\n",
    "\n",
    "def evaluate_policy_fitness(env, policy, n_episodes=100):\n",
    "    \n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "        \n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "\n",
    "def gen_random_policy():\n",
    "    \n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "\n",
    "def crossover(policy1, policy2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ----------\n",
    "    policy1: parent 1\n",
    "    policy2: parent 2\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    child_policy: offspring\n",
    "    \"\"\"\n",
    "    child_policy = policy1.copy()\n",
    "    # IMPLEMENT!\n",
    "    # generate a child policy from cross-over of the parents\n",
    "    j = np.random.randint(1, len(policy1) -2)\n",
    "    child_policy[j:] = policy2[j:]\n",
    "    return child_policy\n",
    "\n",
    "\n",
    "def mutation(policy, p=0.05):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    policy -- policy to be mutated\n",
    "    p      -- mutation probability\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    new_policy -- a mutated version of 'policy'\n",
    "    \"\"\"\n",
    "\n",
    "    # IMPLEMENT!\n",
    "    # mutate new_policy  \n",
    "    ids = np.random.randint(1, len(policy) -1, 2)\n",
    "    policy[ids[0]], policy[ids[1]] = policy[ids[1]], policy[ids[0]]\n",
    "\n",
    "    return policy\n",
    "\n",
    "#set gym env\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.seed(0)\n",
    "\n",
    "## Policy search\n",
    "n_policy = 100\n",
    "n_gens = 20 # number of generations\n",
    "start = time.time()\n",
    "\n",
    "# initialize the population with random policies, i.e. a policy population\n",
    "policy_pop = []\n",
    "for _ in range(n_policy):\n",
    "    policy_pop.append(gen_random_policy())\n",
    "\n",
    "# to initialize previous policy population, just use the initial policy_pop\n",
    "prev_policy_pop = policy_pop.copy()\n",
    "\n",
    "for gen in range(1, n_gens+1):\n",
    "    \n",
    "    # evaluate fitness AND diversity of the policies\n",
    "    policy_scores = []\n",
    "    for policy in policy_pop:\n",
    "        policy_scores.append(evaluate_policy_diversity_fitness(env, policy, prev_policy_pop))    \n",
    "        \n",
    "    # Print the best score of each generation to see improvement\n",
    "    print('Generation %d: max fitness & diversity score = %0.2f' %(gen, max(policy_scores)))\n",
    "    \n",
    "    # rank the policies based on fitness\n",
    "    policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "    # select the top 5 policies: no cross-over\n",
    "    elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "    # get selection probabilities based on the policy scores\n",
    "    select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
    "    \n",
    "    \n",
    "    child_set = []\n",
    "    for i in range(n_policy - 5):\n",
    "        # randomly select parents weighted towards high select_probs\n",
    "        parent1 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        parent2 = policy_pop[np.random.choice(range(n_policy), p=select_probs)]\n",
    "        # generate a child from cross-over btw parents\n",
    "        child = crossover(parent1, parent2)\n",
    "        child_set.append(child)  \n",
    "        \n",
    "    # mutate the children\n",
    "    mutated_list = []\n",
    "    for child in child_set:\n",
    "        mutated = mutation(child)\n",
    "        mutated_list.append(mutated)\n",
    "\n",
    "    # add the elite set (no cross-over) to the new policy population\n",
    "    policy_pop = elite_set\n",
    "    # add the mutated child set\n",
    "    policy_pop += mutated_list\n",
    "\n",
    "# evaluate policies only on fitness\n",
    "policy_scores = []\n",
    "for policy in policy_pop:\n",
    "    policy_scores.append(evaluate_policy_fitness(env, policy))\n",
    "    \n",
    "best_policy = policy_pop[np.argmax(policy_scores)]\n",
    "\n",
    "end = time.time()\n",
    "print('Best policy fitness score = %0.2f. Time taken = %4.4f'\n",
    "        %(np.max(policy_scores), (end-start)))    \n",
    "\n",
    "## Evaluation\n",
    "env = wrappers.Monitor(env, '/tmp/frozenlake1', force=True)\n",
    "for _ in range(200):\n",
    "    run_episode(env, best_policy)\n",
    "env.close()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: max fitness & diversity score = 0.49\n",
      "Generation 2: max fitness & diversity score = 0.49\n",
      "Generation 3: max fitness & diversity score = 0.48\n",
      "Generation 4: max fitness & diversity score = 0.46\n",
      "Generation 5: max fitness & diversity score = 0.47\n",
      "Generation 6: max fitness & diversity score = 0.48\n",
      "Generation 7: max fitness & diversity score = 0.48\n",
      "Generation 8: max fitness & diversity score = 0.70\n",
      "Generation 9: max fitness & diversity score = 0.69\n",
      "Generation 10: max fitness & diversity score = 0.71\n",
      "Generation 11: max fitness & diversity score = 0.69\n",
      "Generation 12: max fitness & diversity score = 0.72\n",
      "Generation 13: max fitness & diversity score = 0.77\n",
      "Generation 14: max fitness & diversity score = 0.78\n",
      "Generation 15: max fitness & diversity score = 0.79\n",
      "Generation 16: max fitness & diversity score = 0.81\n",
      "Generation 17: max fitness & diversity score = 0.79\n",
      "Generation 18: max fitness & diversity score = 0.80\n",
      "Generation 19: max fitness & diversity score = 0.80\n",
      "Generation 20: max fitness & diversity score = 0.78\n",
      "Best policy fitness score = 0.76. Time taken = 57.7457\n"
     ]
    }
   ]
  }
 ]
}